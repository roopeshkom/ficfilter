{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from collections import OrderedDict\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the url we are scraping from (Assumes user knowledge of sorting/rating vals)\n",
    "def get_story_url(genre, series, sort, rating):\n",
    "    page_flags = f'?&srt={sort}&r={rating}&p='\n",
    "    page_url = f'https://www.fanfiction.net/{genre}/{series}/{page_flags}'\n",
    "    return page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yields an iterator for a given story url (Assumes html indexers are unique identifiers)\n",
    "def story_gen(page_url, pages_wanted=None):    \n",
    "    num_pages = get_last_page(page_url)\n",
    "    num_pages = num_pages if not pages_wanted else min(num_pages, pages_wanted)\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        print(f'Scraping page {page}/{num_pages}...')\n",
    "        page_text = requests.get(page_url + str(page)).text\n",
    "        soup = BeautifulSoup(page_text, 'lxml')\n",
    "        \n",
    "        stories = [[title.text, title['href'], author.text] + stats.text.split(' - ') \\\n",
    "                    for (title, author, stats) in \\\n",
    "                    zip(soup.find_all(class_ = 'stitle'), \\\n",
    "                        soup.find_all('a', href=re.compile(r'/u/')), \\\n",
    "                        soup.find_all(class_ = 'z-padtop2 xgray'))]\n",
    "        \n",
    "        for story in stories:\n",
    "            yield story_lexer(story)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gives types for a tokenized story list\n",
    "def story_lexer(story):\n",
    "    fields = OrderedDict()\n",
    "    fields['name'] = story.pop(0)\n",
    "    fields['link'] = f'https://www.fanfiction.net{story.pop(0)}'\n",
    "    fields['author'] = story.pop(0)\n",
    "    fields['rating'] = story.pop(0)[7:]\n",
    "    fields['language'] = story.pop(0)\n",
    "    fields['genre'] = story.pop(0) if story[0][:10] != 'Chapters: ' else None\n",
    "    fields['chapters'] = int(story.pop(0)[10:].replace(',', ''))\n",
    "    fields['words'] = int(story.pop(0)[7:].replace(',', ''))\n",
    "    fields['reviews'] = int(story.pop(0)[9:].replace(',', '')) if story[0][:9] == 'Reviews: ' else None\n",
    "    fields['favs'] = int(story.pop(0)[6:].replace(',', '')) if story[0][:6] == 'Favs: ' else None\n",
    "    fields['follows'] = int(story.pop(0)[9:].replace(',', '')) if story[0][:9] == 'Follows: ' else None\n",
    "    fields['updated'] = story.pop(0)[9:] if story[0][:9] == 'Updated: ' else None\n",
    "    fields['published'] = story.pop(0)[11:]\n",
    "    fields['characters'] = story.pop(0) if story and story[0] != 'Complete' else None\n",
    "    fields['complete'] = bool(story)\n",
    "    return fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the last page number for a given story url (Assumes at least 50 stories for url)\n",
    "def get_last_page(page_url):\n",
    "    page_text = requests.get(page_url).text\n",
    "    soup = BeautifulSoup(page_text, 'lxml')\n",
    "    last_page = int(soup.find('a', text=re.compile('Last'))['href'].split('=')[-1])\n",
    "    return last_page"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
