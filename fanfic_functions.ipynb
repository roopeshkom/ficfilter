{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the url we are scraping from (Assumes user knowledge of sorting/rating vals)\n",
    "def get_fanfic_url(genre, series, sort, rating):\n",
    "    page_flags = f'?&srt={sort}&r={rating}&p='\n",
    "    page_url = f'https://www.fanfiction.net/{genre}/{series}/{page_flags}'\n",
    "    return page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yields a generator for a given fanfic url (Assumes html indexers are unique identifiers)\n",
    "def fanfic_gen(page_url, pages_wanted=None):\n",
    "    num_pages = get_max_pages(page_url)\n",
    "    num_pages = num_pages if not pages_wanted else min(num_pages, pages_wanted)\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_text = requests.get(page_url + str(page)).text\n",
    "        soup = BeautifulSoup(page_text, 'lxml')\n",
    "        \n",
    "        headings = [[title.text, title['href'], author.text] for (title, author) in \\\n",
    "                    zip(soup.find_all(class_ = 'stitle'), \\\n",
    "                        soup.find_all('a', href=re.compile(r'/u/')))]\n",
    "        stories = [heading + stats.text.split(' - ') for (heading, stats) in \\\n",
    "                   zip(headings, soup.find_all(class_ = 'z-padtop2 xgray'))]\n",
    "                \n",
    "        for story in stories:\n",
    "            yield story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Returns the maximum page number for a given fanfic url (Assumes at least 50 stories for url)\n",
    "def get_max_pages(page_url):\n",
    "    page_text = requests.get(page_url).text\n",
    "    soup = BeautifulSoup(page_text, 'lxml')\n",
    "    pages = int(soup.find('a', text=re.compile('Last'))['href'].split('=')[-1])\n",
    "    return pages"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
